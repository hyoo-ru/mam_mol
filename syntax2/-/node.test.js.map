{"version":3,"sources":["../../../mam.ts","../../../mol/fail/fail.ts","../../../mol/syntax2/syntax2.ts"],"names":[],"mappings":";;AAAA,KAAK,CAAC,eAAe,GAAG,EAAE,CAAC;AAK3B,IAAU,CAAC,CAMV;AAND,WAAU,CAAC;AAMX,CAAC,EANS,CAAC,KAAD,CAAC,QAMV;AAED,MAAM,CAAC,OAAO,GAAG,CAAC,CAAA;;;;;;;;;;;;;;;;;ACblB,IAAU,CAAC,CAMV;AAND,WAAU,CAAC;IAEV,SAAgB,SAAS,CAAE,KAAW;QACrC,MAAM,KAAK,CAAA;IACZ,CAAC;IAFe,WAAS,YAExB,CAAA;AAEF,CAAC,EANS,CAAC,KAAD,CAAC,QAMV;;;;ACND,IAAU,CAAC,CA+EV;AA/ED,WAAU,CAAC;IAGV,MAAa,YAAY;QAGhB;QADR,YACQ,MAAe;YAAf,WAAM,GAAN,MAAM,CAAS;YAGtB,KAAK,IAAI,IAAI,IAAI,MAAM,EAAG,CAAC;gBAC1B,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC;oBACf,IAAI,EAAG,IAAI;oBACX,MAAM,EAAG,MAAM,CAAE,IAAI,CAAE;oBACvB,IAAI,EAAG,MAAM,CAAE,KAAK,GAAG,MAAM,CAAE,IAAI,CAAE,CAAC,MAAM,CAAE,CAAC,IAAI,CAAE,EAAE,CAAG,CAAC,MAAM,GAAG,CAAC;iBACrE,CAAC,CAAA;YACH,CAAC;YAED,MAAM,KAAK,GAAG,GAAG,GAAG,IAAI,CAAC,KAAK,CAAC,GAAG,CAAE,IAAI,CAAC,EAAE,CAAC,IAAI,CAAC,MAAM,CAAC,MAAM,CAAE,CAAC,IAAI,CAAE,KAAK,CAAE,GAAG,GAAG,CAAA;YACpF,IAAI,CAAC,MAAM,GAAG,MAAM,CAAE,mBAAoB,KAAM,aAAa,EAAG,KAAK,CAAE,CAAA;QAExE,CAAC;QAED,KAAK,GAAG,EAIN,CAAA;QAEF,MAAM,CAAS;QAEf,QAAQ,CACP,IAAa,EACb,MAAwF;YAGxF,IAAI,GAAG,GAAG,CAAC,CAAA;YAEX,MAAM,EAAG,OAAO,GAAG,GAAG,IAAI,CAAC,MAAM,EAAG,CAAC;gBAEpC,MAAM,KAAK,GAAG,GAAG,CAAA;gBAEjB,IAAI,CAAC,MAAM,CAAC,SAAS,GAAG,KAAK,CAAA;gBAC7B,IAAI,KAAK,GAAG,IAAI,CAAC,MAAM,CAAC,IAAI,CAAE,IAAI,CAAG,CAAA;gBAErC,GAAG,GAAG,IAAI,CAAC,MAAM,CAAC,SAAS,CAAA;gBAC3B,IAAI,KAAK,KAAK,GAAG;oBAAG,MAAM,IAAI,KAAK,CAAE,aAAa,CAAE,CAAA;gBAEpD,IAAI,MAAM,GAAG,KAAK,CAAE,CAAC,CAAE,CAAA;gBACvB,IAAI,MAAM;oBAAG,MAAM,CAAE,EAAE,EAAG,MAAM,EAAG,CAAE,MAAM,CAAE,EAAG,KAAK,CAAE,CAAA;gBAEvD,IAAI,MAAM,GAAG,KAAK,CAAE,CAAC,CAAE,CAAA;gBACvB,IAAI,CAAC,MAAM;oBAAG,SAAQ;gBAEtB,IAAI,MAAM,GAAG,CAAC,CAAA;gBACd,KAAK,IAAI,IAAI,IAAI,IAAI,CAAC,KAAK,EAAG,CAAC;oBAE9B,IAAI,KAAK,CAAE,MAAM,GAAG,CAAC,CAAE,EAAG,CAAC;wBAC1B,MAAM,CAAE,IAAI,CAAC,IAAI,EAAG,MAAM,EAAG,KAAK,CAAC,KAAK,CAAE,MAAM,EAAE,MAAM,GAAG,IAAI,CAAC,IAAI,CAAE,EAAG,KAAK,GAAG,MAAM,CAAC,MAAM,CAAE,CAAA;wBAChG,SAAS,MAAM,CAAA;oBAChB,CAAC;oBAED,MAAM,IAAI,IAAI,CAAC,IAAI,GAAG,CAAC,CAAA;gBACxB,CAAC;gBAED,SAAS,CAAE,IAAI,KAAK,CAAE,wBAAwB,CAAE,CAAE,CAAA;YAEnD,CAAC;QAEF,CAAC;QAED,KAAK,CACJ,IAAa,EACb,QAA6G;YAE7G,IAAI,CAAC,QAAQ,CAAE,IAAI,EAAG,CAAE,IAAI,EAAG,GAAG,IAAI,EAAE,EAAE,CAAC,QAAQ,CAAE,IAAI,CAAE,CAAE,GAAI,IAAI,CAAE,CAAE,CAAA;QAC1E,CAAC;KAED;IA1EY,cAAY,eA0ExB,CAAA;AAEF,CAAC,EA/ES,CAAC,KAAD,CAAC,QA+EV;;","sourcesContent":["Error.stackTraceLimit = 50;\n\ndeclare let _$_: { new(): {} } & typeof globalThis\ndeclare class $ extends _$_ {}\n\nnamespace $ {\n\texport type $ = typeof $$\n\texport declare class $$ extends $ {}\n\tnamespace $$ {\n\t\texport type $$ = $\n\t}\n}\n\nmodule.exports = $\n","namespace $ {\n\n\texport function $mol_fail( error : any ) : never {\n\t\tthrow error\n\t}\n\n}\n","namespace $ {\n\n\t/** Creates lexer by dictionary of lexems. Lexem that started first wins. Then lexem that declared earlier wins. Use regexp capture to take parts of token. */\n\texport class $mol_syntax2< Lexems extends { [ name : string ] : RegExp } = {} > {\n\t\t\n\t\tconstructor(\n\t\t\tpublic lexems : Lexems\n\t\t) {\n\n\t\t\tfor( let name in lexems ) {\n\t\t\t\tthis.rules.push({\n\t\t\t\t\tname : name ,\n\t\t\t\t\tregExp : lexems[ name ] ,\n\t\t\t\t\tsize : RegExp( '^$|' + lexems[ name ].source ).exec( '' )!.length - 1 , \n\t\t\t\t})\n\t\t\t}\n\n\t\t\tconst parts = '(' + this.rules.map( rule => rule.regExp.source ).join( ')|(' ) + ')'\n\t\t\tthis.regexp = RegExp( `([\\\\s\\\\S]*?)(?:(${ parts })|$(?![^]))` , 'gmu' ) \n\t\t\t\n\t\t}\n\t\t\n\t\trules = [] as Array<{\n\t\t\tregExp : RegExp ,\n\t\t\tname : string ,\n\t\t\tsize : number\n\t\t}>\n\t\t\n\t\tregexp : RegExp\n\n\t\ttokenize(\n\t\t\ttext : string ,\n\t\t\thandle : ( name : string , found : string , chunks : string[] , offset : number )=> void ,\n\t\t) {\n\t\t\t\n\t\t\tlet end = 0\n\t\t\t\t\n\t\t\tlexing : while( end < text.length ) {\n\n\t\t\t\tconst start = end\n\n\t\t\t\tthis.regexp.lastIndex = start\n\t\t\t\tvar found = this.regexp.exec( text )!\n\t\t\t\t\n\t\t\t\tend = this.regexp.lastIndex\n\t\t\t\tif( start === end ) throw new Error( 'Empty token' )\n\t\t\t\t\n\t\t\t\tvar prefix = found[ 1 ]\n\t\t\t\tif( prefix ) handle( '' , prefix , [ prefix ] , start )\n\t\t\t\t\n\t\t\t\tvar suffix = found[ 2 ]\n\t\t\t\tif( !suffix ) continue\n\n\t\t\t\tlet offset = 4\n\t\t\t\tfor( let rule of this.rules ) {\n\n\t\t\t\t\tif( found[ offset - 1 ] ) {\n\t\t\t\t\t\thandle( rule.name , suffix , found.slice( offset, offset + rule.size ) , start + prefix.length )\n\t\t\t\t\t\tcontinue lexing\n\t\t\t\t\t}\n\n\t\t\t\t\toffset += rule.size + 1\n\t\t\t\t}\n\n\t\t\t\t$mol_fail( new Error( '$mol_syntax2 is broken' ) )\n\n\t\t\t}\n\n\t\t}\n\t\t\n\t\tparse(\n\t\t\ttext : string ,\n\t\t\thandlers : { [ key in keyof Lexems | '' ] : ( found : string , chunks : string[] , offset : number )=> void } ,\n\t\t) : void {\n\t\t\tthis.tokenize( text , ( name , ...args )=> handlers[ name ]( ... args ) )\n\t\t}\n\n\t}\n\t\n}\n"]}